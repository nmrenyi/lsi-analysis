{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'toy100'\n",
    "dim = 100\n",
    "random_seed = 9999744\n",
    "config = f'data_{dataset}-dim_{dim}-rand_{random_seed}'\n",
    "save_dir = f'../result/{config}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_doc_sparse = sparse.load_npz(os.path.join(save_dir, f'termDocSparse.npz'))\n",
    "term_array = np.load(os.path.join(save_dir, f'termArray.npy'))\n",
    "doc_array  = np.load(os.path.join(save_dir, f'docArray.npy'))\n",
    "with open(os.path.join(save_dir, 'term-doc.json'), mode='r', encoding='utf8') as f:\n",
    "    term_doc_text = json.load(f)\n",
    "    terms = term_doc_text['terms']\n",
    "    docs = term_doc_text['docs']\n",
    "with open(os.path.join(save_dir, 'frobenius-norm-approx-raw.txt'), mode='r', encoding='utf8') as f:\n",
    "    frob_norm = float(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('term_doc_sparse.shape:', term_doc_sparse.shape)\n",
    "print('term_array.shape', term_array.shape)\n",
    "print('doc_array.shape', doc_array.shape)\n",
    "print('#term_text:', len(terms))\n",
    "print('#doc_text:', len(docs))\n",
    "print('feature_dim:', doc_array.shape[1])\n",
    "print('frobenius norm:', frob_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 8\n",
    "arr = term_array\n",
    "np.dot(arr[t], arr[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = ['中华民族', '中国人']\n",
    "other_words = ['依法', '音乐']\n",
    "words = synonyms + other_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(target_in_cnt, target_num, target_dict):\n",
    "    target_doc_list = []\n",
    "    for i in range(len(docs)):\n",
    "        in_cnt = 0\n",
    "        # WARNING: words are always matched from the begininig of the list\n",
    "        for w in words:\n",
    "            if w in docs[i]:\n",
    "                in_cnt += 1\n",
    "        if in_cnt == target_in_cnt:\n",
    "            target_doc_list.append({\n",
    "                'id': i,\n",
    "                '#appear': target_in_cnt,\n",
    "                'text': docs[i],\n",
    "                'array': doc_array[i],\n",
    "                'type': 'd',\n",
    "            })\n",
    "            if len(target_doc_list) == target_num:\n",
    "                break\n",
    "    if len(target_doc_list) != target_num:\n",
    "        print('***cannot find enough required docs containing {} words. found {}, want {}'.format(target_in_cnt, len(target_doc_list), target_num))\n",
    "    target_dict[target_in_cnt] = target_doc_list\n",
    "    return target_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = {}  # key: #words appeared in doc, value: list of doc info\n",
    "doc_dict = get_doc(0, 2, doc_dict)\n",
    "doc_dict = get_doc(1, 2, doc_dict)\n",
    "doc_dict = get_doc(2, 2, doc_dict)\n",
    "doc_dict = get_doc(3, 1, doc_dict)\n",
    "doc_dict = get_doc(4, 1, doc_dict)\n",
    "doc_info_list = [d for i in range(5) for d in doc_dict[i]]\n",
    "for i, d in enumerate(doc_info_list):\n",
    "    d['index'] = i\n",
    "print('#found docs:', len(doc_info_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert items in terms and docs list are unique, respectively\n",
    "words_info_list = []\n",
    "for word in words:\n",
    "    index = terms.index(word)\n",
    "    words_info_list.append({\n",
    "        'id': index,\n",
    "        'index': len(words_info_list),\n",
    "        'text': word,\n",
    "        'array': term_array[index],\n",
    "        'type': 'w',\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_list = words_info_list + doc_info_list\n",
    "similarity_list = [{\n",
    "                    'item1': x['type'] + str(x['index']), \n",
    "                    'item2': y['type'] + str(y['index']), \n",
    "                    'similarity': np.dot(x['array'], y['array']) / np.linalg.norm(x['array'], ord=2) / np.linalg.norm(y['array'], ord=2),\n",
    "                    } \n",
    "                    for x in info_list for y in info_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame(similarity_list)\n",
    "similarity_pivot_df = similarity_df.pivot(index='item1', columns='item2', values='similarity')\n",
    "col_order = ['w' + str(i) for i in range(len(words_info_list))] + ['d' + str(i) for i in range(len(doc_info_list))]\n",
    "similarity_pivot_df = similarity_pivot_df.reindex(col_order, axis=0).reindex(col_order, axis=1)\n",
    "similarity_pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_pivot_df.to_csv(os.path.join(save_dir, 'similarity.csv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(similarity_pivot_df, cmap=\"YlGnBu\", square=True)\n",
    "ax.xaxis.tick_top()\n",
    "ax.set(xlabel=None)\n",
    "ax.set(ylabel=None)\n",
    "plt.title('\\n'.join(['Cosine Similarity Heatmap Between Words and Docs', f'dataset={dataset}, dim={dim}, random_seed={random_seed}']))\n",
    "plt.savefig(os.path.join(save_dir, 'cos-sim-heatmap.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=random_seed)\n",
    "# vec2d = tsne.fit_transform([x['array'] / np.linalg.norm(x['array'], ord=2) for x in info_list])\n",
    "vec2d = tsne.fit_transform([x['array'] for x in info_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing vectors with legend. \n",
    "# ref: https://stackoverflow.com/questions/42281966/how-to-plot-vectors-in-python-using-matplotlib\n",
    "rows,cols = vec2d.T.shape\n",
    "\n",
    "# Get absolute maxes for axis ranges to center origin\n",
    "# This is optional\n",
    "maxes = 1.1*np.amax(abs(vec2d), axis = 0)\n",
    "\n",
    "for i,l in enumerate(range(0,cols)):\n",
    "    xs = [0,vec2d[i,0]]\n",
    "    ys = [0,vec2d[i,1]]\n",
    "    plt.plot(xs,ys)\n",
    "\n",
    "# plt.plot(0,0,'ok') #<-- plot a black point at the origin\n",
    "plt.axis('equal')  #<-- set the axes to the same scale\n",
    "plt.xlim([-maxes[0],maxes[0]]) #<-- set the x axis limits\n",
    "plt.ylim([-maxes[1],maxes[1]]) #<-- set the y axis limits\n",
    "plt.legend([x['type'] + str(x['index']) for x in info_list]) #<-- give a legend\n",
    "plt.grid(b=True, which='major') #<-- plot grid lines\n",
    "plt.title('\\n'.join(['Word and Doc Embedding Visualization with t-SNE', f'dataset={dataset}, dim={dim}, random_seed={random_seed}']))\n",
    "plt.savefig(os.path.join(save_dir, 'word-doc-emb-visualization.pdf'), bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a15ccb50246f7e7797bfc44dccdce90a74059fe81a59d1aaacc8dc8dd0324c5c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
